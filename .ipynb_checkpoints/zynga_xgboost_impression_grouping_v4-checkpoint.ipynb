{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frame Systems Predicting Conversions in Impression Groups in Amazon SageMaker XGBoost\n",
    "_**Supervised Learning with Gradient Boosted Trees: A Binary Prediction Problem With Unbalanced Classes**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Prepration](#Preparation)\n",
    "1. [Data](#Data)\n",
    "    1. [Exploration](#Exploration)\n",
    "    1. [Transformation](#Transformation)\n",
    "1. [Training](#Training)\n",
    "1. [Hosting](#Hosting)\n",
    "1. [Evaluation](#Evaluation)\n",
    "1. [Exentsions](#Extensions)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "Impressions are sampled along with feature data, clicks, and conversions.  A model is built using XgBoost in sagemaker that tries to then predict whether an impression group will convert or not.\n",
    "\n",
    "This notebook presents an example problem to predict if a user will convert   The steps include:\n",
    "\n",
    "* Preparing your Amazon SageMaker notebook\n",
    "* Downloading data from s3 into Amazon SageMaker\n",
    "* Investigating and transforming the data so that it can be fed to Amazon SageMaker algorithms\n",
    "* Estimating a model using the Gradient Boosting algorithm\n",
    "* Evaluating the effectiveness of the model\n",
    "* Setting the model up to make on-going predictions\n",
    "\n",
    "---\n",
    "\n",
    "## Preparation\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role ARN used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "buzzkey = 'zynga'\n",
    "release_version = 'exp'\n",
    "cpa = 10\n",
    "tag_ids = '1,2'\n",
    "bucket = '{}-beeswax'.format(buzzkey)\n",
    "prefix = 'sagemaker/{}-xgboost-impression-grouping'.format(buzzkey)\n",
    " \n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring in the Python libraries that we'll use throughout the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import matplotlib.pyplot as plt                   # For charts and visualizations\n",
    "from IPython.display import Image                 # For displaying images in the notebook\n",
    "from IPython.display import display               # For displaying outputs in the notebook\n",
    "from time import gmtime, strftime                 # For labeling SageMaker models, endpoints, etc.\n",
    "import sys                                        # For writing outputs to notebook\n",
    "import math                                       # For ceiling function\n",
    "import json                                       # For parsing hosting outputs\n",
    "import os                                         # For manipulating filepath names\n",
    "import sagemaker                                  # Amazon SageMaker's Python SDK provides many helper functions\n",
    "from sagemaker.predictor import csv_serializer    # Converts strings for HTTP POST requests on inference\n",
    "import datetime as dt                             #datetime python lib\n",
    "import pytz                                       #timezone adjuster\n",
    "import time                                       #time lib for waiters\n",
    "import uuid                                       #guid generator\n",
    "import s3fs                                       #file helper that helps pandas work in s3\n",
    "import requests                                   #request handler for rest api stuff with Beeswax api\n",
    "from decimal import *                             #converts floats to decimals used for weird dynamoDB stuff\n",
    "from io import StringIO                           #library to convert strings for S3 upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data\n",
    "The demo started with data that was downloaded from [direct marketing dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing) from UCI's ML Repository. I am using a similar setup and model but applying to impression groups.\n",
    "I replaced it with a static file that I generated from AWS Machine Learning located at: 's3://fb-beeswax/brian/ml-prediction/2019-02-22/fb-prediction-2019-02-22-230548.csv'  The file is a random 7 days of impression and conversion data.\n",
    "\n",
    "Set some datetime helpers that might be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = pytz.timezone('EST')\n",
    "\n",
    "today = dt.datetime.now(tz)\n",
    "sevenday = today - dt.timedelta(days=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the query that will be sent to Athena to get the data.  Note that I take a Bernouli sampling of the impression data after filtering out the conversions for a smaller data set of negative data.  This is so I don't over-bais the algorithm with negative data.  Then I take the same columns of all of the conversion data so I have maximum diversity of conversions.  I've heard you want 20% positive data to 80% negative data.  The problem with that is that if conversions are sparse/low then it limits the amount of negative data and limits the diversity of negative data that you feed the model.  I've been shooting for between 5-20% but have mostly been on the lower end.\n",
    "\n",
    "Currently sampling 5% of the table with a 14 day lookback starting 2 days ago (so essentially day -16 thru day -2 from today).  The theory here is to build the model on historical data where the conversion lookback data has been fully baked (or at least for 2 days), and then you would score the previous 2-3 days of impression groups with minimal or no overlay from what the model was built on and what you are trying to score to predict the future.  Goal here is to maximixe the model diversity from recent history with fully baked data to build the model and maximize recency nuances of new inventory and users that are showing up in the past couple days that are likely to show up again tomorrow.\n",
    "\n",
    "** The model builder query uses a straight 14 day lookback from today.\n",
    "** Should check to see what happens when we include more negative data rather than removing it.\n",
    "** Should we target a CVR instead of the presence or absence of conversions, so we control for impression volume?  Or does that minimize the value of large cells?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing platform_device_model due to over 1200 unique values.\n",
    "# Revmoving video_completes, clicks, companion views and companion clicks since those are not valid for the auction\n",
    "# query athena view of impression, click, and joined conversion data for tag_id 3\n",
    "query = \"\"\"\n",
    "select * from (SELECT  conversions,\n",
    "         campaign_id,\n",
    "         impressions,\n",
    "         app_bundle,\n",
    "         ad_position,\n",
    "         geo_country,\n",
    "         platform_browser,\n",
    "         platform_os,\n",
    "         rewarded,\n",
    "         platform_carrier,\n",
    "         platform_device_make,\n",
    "         video_player_size,\n",
    "         content_language,\n",
    "         banner_width,\n",
    "         banner_height,\n",
    "         inventory_source,\n",
    "         inventory_interstitial,\n",
    "         spend\n",
    "FROM \"{buzzkey}-{release_version}\".\"{buzzkey}-model-builder\" where impressions >= 4 and conversions = 0\n",
    "UNION ALL\n",
    "SELECT  conversions,\n",
    "         campaign_id,\n",
    "         impressions,\n",
    "         app_bundle,\n",
    "         ad_position,\n",
    "         geo_country,\n",
    "         platform_browser,\n",
    "         platform_os,\n",
    "         rewarded,\n",
    "         platform_carrier,\n",
    "         platform_device_make,\n",
    "         video_player_size,\n",
    "         content_language,\n",
    "         banner_width,\n",
    "         banner_height,\n",
    "         inventory_source,\n",
    "         inventory_interstitial,\n",
    "         spend\n",
    "FROM \"{buzzkey}-{release_version}\".\"{buzzkey}-model-builder\" where conversions > 0) order by random()\n",
    "\"\"\".format(buzzkey=buzzkey, release_version=release_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goes to the `zynga-exp` database in Athena runs the query job and produces an object that will have the s3:// link with the results in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ID: cc345236-67f1-4636-9d62-3dc848ebe07a\n",
      "query processing for 0 seconds\n"
     ]
    }
   ],
   "source": [
    "# set database to the current project (Zynga/facebook/etc)\n",
    "database = '{}-{}'.format(buzzkey, release_version)\n",
    "#set s3 output file for athena query\n",
    "s3_output = 's3://{}-beeswax/brian/athena/{}/'.format(buzzkey, dt.datetime.now(tz).strftime('%Y-%m-%d-%H%M%S'))\n",
    "\n",
    "#Function for starting athena query\n",
    "def run_query(query, database, s3_output):\n",
    "    client = boto3.client('athena', region_name='us-east-1')\n",
    "    response = client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database\n",
    "            },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': s3_output,\n",
    "            }\n",
    "        )\n",
    "    print('Execution ID: ' + response['QueryExecutionId'])\n",
    "    return response\n",
    "\n",
    "#run athena query and kick back job id\n",
    "job = run_query(query, database, s3_output)\n",
    "\n",
    "job_id = job['QueryExecutionId']\n",
    "client = boto3.client('athena', region_name='us-east-1')\n",
    "res = client.get_query_execution(QueryExecutionId= job_id)\n",
    "x = 0\n",
    "\n",
    "# wait for athena to return results\n",
    "while res['QueryExecution']['Status']['State'] != 'SUCCEEDED':\n",
    "    print(\"query processing for %s seconds\" %str(x))\n",
    "    time.sleep(10)\n",
    "    x = x + 10\n",
    "    res = client.get_query_execution(QueryExecutionId= job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://zynga-beeswax/brian/athena/2019-03-22-155028/cc345236-67f1-4636-9d62-3dc848ebe07a.csv'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#set output location for query results\n",
    "output = res['QueryExecution']['ResultConfiguration']['OutputLocation']\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets read this into a Pandas data frame and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversions</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>impressions</th>\n",
       "      <th>app_bundle</th>\n",
       "      <th>ad_position</th>\n",
       "      <th>geo_country</th>\n",
       "      <th>platform_browser</th>\n",
       "      <th>platform_os</th>\n",
       "      <th>rewarded</th>\n",
       "      <th>platform_carrier</th>\n",
       "      <th>platform_device_make</th>\n",
       "      <th>video_player_size</th>\n",
       "      <th>content_language</th>\n",
       "      <th>banner_width</th>\n",
       "      <th>banner_height</th>\n",
       "      <th>inventory_source</th>\n",
       "      <th>inventory_interstitial</th>\n",
       "      <th>spend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>uk.co.aifactory.heartsfree</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320</td>\n",
       "      <td>480</td>\n",
       "      <td>MOPUB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>com.freegame.solitaire.basic2</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>GOOGLE_ADX</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>com.scopely.yux</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>GOOGLE_ADX</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>game.bubble.shooter.dragon.pop</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LG</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320</td>\n",
       "      <td>480</td>\n",
       "      <td>APP_LOVIN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>com.fingerstudios.solitaire.classic</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>GOOGLE_ADX</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>com.fingerstudios.solitaire.classic</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>GOOGLE_ADX</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>com.classic.solitaire</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>AT&amp;T</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320</td>\n",
       "      <td>480</td>\n",
       "      <td>MOPUB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>com.karmangames.pinochle</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>GOOGLE_ADX</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>uk.co.aifactory.backgammonfree</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>Verizon</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>480</td>\n",
       "      <td>320</td>\n",
       "      <td>MOPUB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>com.classic.solitaire</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>T-Mobile</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320</td>\n",
       "      <td>480</td>\n",
       "      <td>MOPUB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>uk.co.aifactory.chessfree</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>AT&amp;T</td>\n",
       "      <td>LG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320</td>\n",
       "      <td>480</td>\n",
       "      <td>MOPUB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>bubble.shoot.fruit.splash.game2</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZTE</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320</td>\n",
       "      <td>480</td>\n",
       "      <td>APP_LOVIN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>com.gsn.android.tripeaks</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>480</td>\n",
       "      <td>320</td>\n",
       "      <td>MOPUB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>com.classic.solitaire</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>Verizon</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320</td>\n",
       "      <td>480</td>\n",
       "      <td>MOPUB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>com.freegame.solitaire.basic2</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>GOOGLE_ADX</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>uk.co.aifactory.spadesfree</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>T-Mobile</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320</td>\n",
       "      <td>480</td>\n",
       "      <td>MOPUB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>uk.co.aifactory.backgammonfree</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>480</td>\n",
       "      <td>320</td>\n",
       "      <td>MOPUB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>com.classic.solitaire</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>T-Mobile</td>\n",
       "      <td>Blu</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320</td>\n",
       "      <td>480</td>\n",
       "      <td>MOPUB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>com.classic.solitaire</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>AT&amp;T</td>\n",
       "      <td>Yulong</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320</td>\n",
       "      <td>480</td>\n",
       "      <td>MOPUB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>com.gsn.android.tripeaks</td>\n",
       "      <td>ABOVE_THE_FOLD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>-1</td>\n",
       "      <td>Verizon</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>480</td>\n",
       "      <td>320</td>\n",
       "      <td>MOPUB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>899 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     conversions  campaign_id  impressions  \\\n",
       "0              0            5            1   \n",
       "1              0            5            2   \n",
       "2              0            5            1   \n",
       "3              0            5            2   \n",
       "4              0            5            1   \n",
       "5              0            5            1   \n",
       "6              0            5            1   \n",
       "7              0            5            1   \n",
       "8              0            5            1   \n",
       "9              0            5            1   \n",
       "..           ...          ...          ...   \n",
       "889            0            5            1   \n",
       "890            0            5            1   \n",
       "891            0            5            1   \n",
       "892            0            5            1   \n",
       "893            0            5            1   \n",
       "894            0            5            1   \n",
       "895            0            5            1   \n",
       "896            0            5            1   \n",
       "897            0            5            1   \n",
       "898            0            5            1   \n",
       "\n",
       "                              app_bundle     ad_position geo_country  \\\n",
       "0             uk.co.aifactory.heartsfree  ABOVE_THE_FOLD         USA   \n",
       "1          com.freegame.solitaire.basic2  ABOVE_THE_FOLD         USA   \n",
       "2                        com.scopely.yux  ABOVE_THE_FOLD         USA   \n",
       "3         game.bubble.shooter.dragon.pop  ABOVE_THE_FOLD         USA   \n",
       "4    com.fingerstudios.solitaire.classic  ABOVE_THE_FOLD         USA   \n",
       "5    com.fingerstudios.solitaire.classic  ABOVE_THE_FOLD         USA   \n",
       "6                  com.classic.solitaire  ABOVE_THE_FOLD         USA   \n",
       "7               com.karmangames.pinochle  ABOVE_THE_FOLD         USA   \n",
       "8         uk.co.aifactory.backgammonfree  ABOVE_THE_FOLD         USA   \n",
       "9                  com.classic.solitaire  ABOVE_THE_FOLD         USA   \n",
       "..                                   ...             ...         ...   \n",
       "889            uk.co.aifactory.chessfree  ABOVE_THE_FOLD         USA   \n",
       "890      bubble.shoot.fruit.splash.game2  ABOVE_THE_FOLD         USA   \n",
       "891             com.gsn.android.tripeaks  ABOVE_THE_FOLD         USA   \n",
       "892                com.classic.solitaire  ABOVE_THE_FOLD         USA   \n",
       "893        com.freegame.solitaire.basic2  ABOVE_THE_FOLD         USA   \n",
       "894           uk.co.aifactory.spadesfree  ABOVE_THE_FOLD         USA   \n",
       "895       uk.co.aifactory.backgammonfree  ABOVE_THE_FOLD         USA   \n",
       "896                com.classic.solitaire  ABOVE_THE_FOLD         USA   \n",
       "897                com.classic.solitaire  ABOVE_THE_FOLD         USA   \n",
       "898             com.gsn.android.tripeaks  ABOVE_THE_FOLD         USA   \n",
       "\n",
       "    platform_browser platform_os  rewarded platform_carrier  \\\n",
       "0      Chrome Mobile     Android        -1              NaN   \n",
       "1      Chrome Mobile     Android        -1              NaN   \n",
       "2      Chrome Mobile     Android        -1              NaN   \n",
       "3      Chrome Mobile     Android        -1              NaN   \n",
       "4      Chrome Mobile     Android        -1              NaN   \n",
       "5      Chrome Mobile     Android        -1              NaN   \n",
       "6      Chrome Mobile     Android        -1             AT&T   \n",
       "7      Chrome Mobile     Android        -1              NaN   \n",
       "8      Chrome Mobile     Android        -1          Verizon   \n",
       "9      Chrome Mobile     Android        -1         T-Mobile   \n",
       "..               ...         ...       ...              ...   \n",
       "889    Chrome Mobile     Android        -1             AT&T   \n",
       "890    Chrome Mobile     Android        -1              NaN   \n",
       "891    Chrome Mobile     Android        -1              NaN   \n",
       "892    Chrome Mobile     Android        -1          Verizon   \n",
       "893    Chrome Mobile     Android        -1              NaN   \n",
       "894    Chrome Mobile     Android        -1         T-Mobile   \n",
       "895    Chrome Mobile     Android        -1              NaN   \n",
       "896    Chrome Mobile     Android        -1         T-Mobile   \n",
       "897    Chrome Mobile     Android        -1             AT&T   \n",
       "898    Chrome Mobile     Android        -1          Verizon   \n",
       "\n",
       "    platform_device_make video_player_size content_language  banner_width  \\\n",
       "0                Samsung               NaN              NaN           320   \n",
       "1                     LG               NaN               en            -1   \n",
       "2                Samsung               NaN               en            -1   \n",
       "3                     LG                 M              NaN           320   \n",
       "4                Samsung               NaN               en            -1   \n",
       "5                Samsung               NaN               en            -1   \n",
       "6                Samsung                 M              NaN           320   \n",
       "7                     LG               NaN               en            -1   \n",
       "8                Samsung               NaN              NaN           480   \n",
       "9               Motorola                 M              NaN           320   \n",
       "..                   ...               ...              ...           ...   \n",
       "889                   LG               NaN              NaN           320   \n",
       "890                  ZTE                 M              NaN           320   \n",
       "891              Samsung                 M              NaN           480   \n",
       "892              Samsung                 M              NaN           320   \n",
       "893                   LG               NaN               en            -1   \n",
       "894              Samsung               NaN              NaN           320   \n",
       "895              Samsung               NaN              NaN           480   \n",
       "896                  Blu                 M              NaN           320   \n",
       "897               Yulong                 M              NaN           320   \n",
       "898              Samsung                 M              NaN           480   \n",
       "\n",
       "     banner_height inventory_source  inventory_interstitial     spend  \n",
       "0              480            MOPUB                       1  0.011779  \n",
       "1               -1       GOOGLE_ADX                       1  0.015212  \n",
       "2               -1       GOOGLE_ADX                       1  0.003967  \n",
       "3              480        APP_LOVIN                       1  0.011730  \n",
       "4               -1       GOOGLE_ADX                       1  0.007606  \n",
       "5               -1       GOOGLE_ADX                       1  0.006886  \n",
       "6              480            MOPUB                       1  0.007186  \n",
       "7               -1       GOOGLE_ADX                       1  0.012345  \n",
       "8              320            MOPUB                       1  0.006166  \n",
       "9              480            MOPUB                       1  0.007186  \n",
       "..             ...              ...                     ...       ...  \n",
       "889            480            MOPUB                       1  0.006549  \n",
       "890            480        APP_LOVIN                       1  0.005865  \n",
       "891            320            MOPUB                       1  0.004348  \n",
       "892            480            MOPUB                       1  0.007186  \n",
       "893             -1       GOOGLE_ADX                       1  0.007606  \n",
       "894            480            MOPUB                       1  0.006432  \n",
       "895            320            MOPUB                       1  0.006602  \n",
       "896            480            MOPUB                       1  0.007186  \n",
       "897            480            MOPUB                       1  0.007186  \n",
       "898            320            MOPUB                       1  0.004784  \n",
       "\n",
       "[899 rows x 18 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(output)\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 20)         # Keep the output on one page\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the data.  At a high level, we can see:\n",
    "\n",
    "* We have a little under 9k records which is a sampling of a total of 17m over 14 days with 387 conversions, and 25 features for each customer I randomly sampled this out of about 1.2m records using a Bernouli sampling method but then randomly stuffed conversion data back into it to weight it properly to avoid bias.\n",
    "* The features are mixed; some numeric, some categorical\n",
    "\n",
    "_**Specifics on each of the features:**_\n",
    "\n",
    "*Quantitative Stats of User Ad Interaction:*\n",
    "* `conversions`: Binary : Conversions is binary and indicates whether the impression resulted in a conversion or not.\n",
    "* `impressions`: Numerical: Count of impressions shown in that cell.\n",
    "* `clicks`: Numeric: Count of clicks on that ad. \n",
    "* `companion_views`: Numeric: Views of companion banner that showed with the video. \n",
    "* `companion_clicks`: Numeric: Count of clicks on the companion ad.\n",
    "* `video_completes`: Numeric: Number of times a user completed the video.\n",
    "* `spend`: Numeric: US dollars spent on the impressions. (micros/1000)\n",
    "\n",
    "*Geographic:*\n",
    "* ##Removed## `geo-region`: Categorical: Country/State(province)\n",
    "* `geo-country`: Categorical: Country i.e. USA, CA\n",
    "\n",
    "*Impression Attributes:*\n",
    "* `app_bundle`: Categorical: alpha numberic identifier for the app the ad was shown on.  iOS is usually numeric, Android is typically text like com.aws.android\n",
    "* `ad_position`: Categorical: enum value from RTB to show where the ad position is i.e. ABOVE_THE_FOLD\n",
    "* `rewarded`: Binary: 1 if it was a rewarded ad and 0 if not\n",
    "* `video_player_size`: Categorical: typicall S, M, L but other things could flow in there.\n",
    "* `banner_width`: Categorical: integer size of ad in pixels wide. (sometimes comes as a float and needs to be converted)\n",
    "* `banner_height`: Categorical: integer size of ad in pixels high. (sometimes comes as a float and needs to be converted)\n",
    "* `inventory_interstitial`: Binary: 1 if it was an interstitial 0 if not.\n",
    "\n",
    " \n",
    "*Campaign information:*\n",
    "* `campaign_id`: Categorical: Beeswax campaign id.\n",
    "\n",
    "*User or Device Attributes:*\n",
    "* `platform_browser`: Categorical: Browser the user has set on their phone i.e. Chrome\n",
    "* `platform_os`: Categorical: android, ios, etc.\n",
    "* `platform_carrier`: Categorical: mobile carrier i.e. Verizon, Sprint, Vodafone.\n",
    "* `platform_device_make`: Categorical: Device manufacturer i.e. Huawei, Apple, LG\n",
    "* `platform_device_model`: Categorical: Model name of the device i.e. Moto G6 Play, SM-J320AZ\n",
    "* `content_language`: Categorical: Default language they have set on their phone or browser. i.e. en, EN (not consistent)\n",
    "\n",
    "*Target variable:*\n",
    "* `conversions`: Binary: 1 means at least one conversion was made, 0 means no conversions in the time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "Let's start exploring the data.  First, let's understand how the features are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=data[column], columns='% observations', normalize='columns'))\n",
    "\n",
    "# Histograms for each numeric features\n",
    "display(data.describe())\n",
    "%matplotlib inline\n",
    "hist = data.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "* A bit over 4% of the values for our target variable `conversions` are \"1\", so most customers did not convert.  This is purposely heavied up so as not to over-bais non-converters.  If we have to choose we'd rather be better at kicking out the losers than picking the needle in the haystack winners.  4% is a bit low but hopefully this should increase over time iteratively if things are working right.  Might need some tuning down the road.\n",
    "\n",
    "Next, let's look at how our features relate to the target that we are attempting to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    if column != 'conversions':\n",
    "        display(pd.crosstab(index=data[column], columns=data['conversions'], normalize='columns'))\n",
    "\n",
    "for column in data.select_dtypes(exclude=['object']).columns:\n",
    "    print(column)\n",
    "    hist = data[[column, 'app_bundle']].hist( bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "* This was done in the demo...not really sure how to read it.\n",
    "\n",
    "Now let's look at how our features relate to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.corr())\n",
    "pd.plotting.scatter_matrix(data, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "* Features vary widely in their relationship with one another.  Some with highly negative correlation, others with highly positive correlation.\n",
    "* Interesting that impressions and spend heaviliy coorelate to conversions but clicks and rewarded negatively coorelated in the original model but now after several days of optimization clicks positively coorelate to conversions.  This kind of makes sense because view thru conversions are so much easier to get it's almost like the click doesn't matter.  I might consider throwing clicks out of the model.\n",
    "* Relationships between features is non-linear and discrete in many cases but spend and impressions are nearly perfectly linear which makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "\n",
    "Cleaning up data is part of nearly every machine learning project.  It arguably presents the biggest risk if done incorrectly and is one of the more subjective aspects in the process.  Several common techniques include:\n",
    "\n",
    "* Handling missing values: Some machine learning algorithms are capable of handling missing values, but most would rather not.  Options include:\n",
    " * Removing observations with missing values: This works well if only a very small fraction of observations have incomplete information.\n",
    " * Removing features with missing values: This works well if there are a small number of features which have a large number of missing values.\n",
    " * Imputing missing values: Entire [books](https://www.amazon.com/Flexible-Imputation-Missing-Interdisciplinary-Statistics/dp/1439868247) have been written on this topic, but common choices are replacing the missing value with the mode or mean of that column's non-missing values.\n",
    "* Converting categorical to numeric: The most common method is one hot encoding, which for each feature maps every distinct value of that column to its own feature which takes a value of 1 when the categorical feature is equal to that value, and 0 otherwise.\n",
    "* Oddly distributed data: Although for non-linear models like Gradient Boosted Trees, this has very limited implications, parametric models like regression can produce wildly inaccurate estimates when fed highly skewed data.  In some cases, simply taking the natural log of the features is sufficient to produce more normally distributed data.  In others, bucketing values into discrete ranges is helpful.  These buckets can then be treated as categorical variables and included in the model when one hot encoded.\n",
    "* Handling more complicated data types: Mainpulating images, text, or data at varying grains is left for other notebook templates.\n",
    "\n",
    "Luckily, some of these aspects have already been handled for us, and the algorithm we are showcasing tends to do well at handling sparse or oddly distributed data.  Therefore, let's keep pre-processing simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['no_previous_contact'] = np.where(data['pdays'] == 999, 1, 0)                                 # Indicator variable to capture when pdays takes a value of 999\n",
    "#data['not_working'] = np.where(np.in1d(data['job'], ['student', 'retired', 'unemployed']), 1, 0)   # Indicator for individuals not actively employed\n",
    "model_data = pd.get_dummies(data)                                                                  # Convert categorical variables to sets of indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another question to ask yourself before building a model is whether certain features will add value in your final use case.  For example, if your goal is to deliver the best prediction, then will you have access to that data at the moment of prediction?  Knowing it's raining is highly predictive for umbrella sales, but forecasting weather far enough out to plan inventory on umbrellas is probably just as difficult as forecasting umbrella sales without knowledge of the weather.  So, including this in your model may give you a false sense of precision.\n",
    "\n",
    "I didn't throw anything out in this first pass but considering throwing out clicks and a few other numeric things that negatively or don't coorelate but at the same time I don't want the algorithm to optimize away from higher CTR cells because if we can get click conversions they are valued more highly in the attribution waterfall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_data = model_data.drop(['duration', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'], axis=1)\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a model whose primary goal is to predict a target value on new data, it is important to understand overfitting.  Supervised learning models are designed to minimize error between their predictions of the target value and actuals, in the data they are given.  This last part is key, as frequently in their quest for greater accuracy, machine learning models bias themselves toward picking up on minor idiosyncrasies within the data they are shown.  These idiosyncrasies then don't repeat themselves in subsequent data, meaning those predictions can actually be made less accurate, at the expense of more accurate predictions in the training phase.\n",
    "\n",
    "The most common way of preventing this is to build models with the concept that a model shouldn't only be judged on its fit to the data it was trained on, but also on \"new\" data.  There are several different ways of operationalizing this, holdout validation, cross-validation, leave-one-out validation, etc.  For our purposes, we'll simply randomly split the data into 3 uneven groups.  The model will be trained on 70% of data, it will then be evaluated on 20% of data to give us an estimate of the accuracy we hope to have on \"new\" data, and 10% will be held back as a final testing dataset which will be used later on.\n",
    "\n",
    "** May want to remove the test split and include it in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])   # Randomly sort the data then split out first 70%, second 20%, and last 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker's XGBoost container expects data in the libSVM or CSV data format.  For this example, we'll stick to CSV.  Note that the first column must be the target variable and the CSV should not include headers.  Also, notice that although repetitive it's easiest to do this after the train|validation|test split rather than before.  This avoids any misalignment issues due to random reordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train.csv', index=False, header=False)\n",
    "validation_data.to_csv('validation.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll copy the file to S3 for Amazon SageMaker's managed training to pickup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training\n",
    "Now we know that most of our features have skewed distributions, some are highly correlated with one another, and some appear to have non-linear relationships with our target variable.  Also, for targeting future prospects, good predictive accuracy is preferred to being able to explain why that prospect was targeted.  Taken together, these aspects make gradient boosted trees a good candidate algorithm.\n",
    "\n",
    "There are several intricacies to understanding the algorithm, but at a high level, gradient boosted trees works by combining predictions from many simple models, each of which tries to address the weaknesses of the previous models.  By doing this the collection of simple models can actually outperform large, complex models.  Other Amazon SageMaker notebooks elaborate on gradient boosting trees further and how they differ from similar algorithms.\n",
    "\n",
    "`xgboost` is an extremely popular, open-source package for gradient boosted trees.  It is computationally powerful, fully featured, and has been successfully used in many machine learning competitions.  Let's start with a simple `xgboost` model, trained using Amazon SageMaker's managed, distributed training framework.\n",
    "\n",
    "First we'll need to specify the ECR container location for Amazon SageMaker's implementation of XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3, which also specify that the content type is CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll need to specify training parameters to the estimator.  This includes:\n",
    "1. The `xgboost` algorithm container\n",
    "1. The IAM role to use\n",
    "1. Training instance type and count\n",
    "1. S3 location for output data\n",
    "1. Algorithm hyperparameters\n",
    "\n",
    "And then a `.fit()` function which specifies:\n",
    "1. S3 location for output data.  In this case we have both a training and validation set which are passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hosting\n",
    "Now that we've trained the `xgboost` algorithm on our data, let's deploy a model that's hosted behind a real-time endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation\n",
    "There are many ways to compare the performance of a machine learning model, but let's start by simply comparing actual to predicted values.  In this case, we're simply predicting whether the user converted (`1`) or not (`0`), which produces a simple confusion matrix.\n",
    "\n",
    "First we'll need to determine how we pass data into and receive data from our endpoint.  Our data is currently stored as NumPy arrays in memory of our notebook instance.  To send it in an HTTP POST request, we'll serialize it as a CSV string and then decode the resulting CSV.\n",
    "\n",
    "*Note: For inference with CSV format, SageMaker XGBoost requires that the data does NOT include the target variable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load some live data from the past 3 days of about 3.7m impression groups where 73 were conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query athena view of impression, click, and joined conversion data for tag_id that is 'active' in the Beeswax UI for that account.\n",
    "query = \"\"\"\n",
    "SELECT  conversions,\n",
    "         campaign_id,\n",
    "         impressions,\n",
    "         clicks,\n",
    "         app_bundle,\n",
    "         ad_position,\n",
    "         geo_country,\n",
    "         platform_browser,\n",
    "         platform_os,\n",
    "         rewarded,\n",
    "         platform_carrier,\n",
    "         platform_device_make,\n",
    "         platform_device_model,\n",
    "         video_player_size,\n",
    "         video_completes,\n",
    "         content_language,\n",
    "         companion_views,\n",
    "         companion_clicks,\n",
    "         banner_width,\n",
    "         banner_height,\n",
    "         inventory_source,\n",
    "         inventory_interstitial,\n",
    "         spend\n",
    "FROM \"{buzzkey}-{release_version}\".\"{buzzkey}-batchprediction\" \n",
    "\"\"\".format(buzzkey=buzzkey, release_version=release_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run query in `{buzzkey}-{release_version}` athena database against a pre-configured view to look back 3 days which is essentially only 2 days because of completeness of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set database to buzzkey\n",
    "database = '{}-{}'.format(buzzkey, release_version)\n",
    "#set s3 output file for athena query\n",
    "s3_output_prediction = 's3://{}-beeswax/brian/athena/prediction/{}/'.format(buzzkey, dt.datetime.now(tz).strftime('%Y-%m-%d-%H%M%S'))\n",
    "\n",
    "#run athena query and kick back job id\n",
    "job = run_query(query, database, s3_output_prediction)\n",
    "\n",
    "job_id = job['QueryExecutionId']\n",
    "res = client.get_query_execution(QueryExecutionId= job_id)\n",
    "x = 0\n",
    "\n",
    "# wait for athena to return results\n",
    "while res['QueryExecution']['Status']['State'] != 'SUCCEEDED':\n",
    "    print(\"it's been {} seconds\".format(str(x)))\n",
    "    time.sleep(10)\n",
    "    x = x + 10\n",
    "    res = client.get_query_execution(QueryExecutionId= job_id)\n",
    "\n",
    "#set output location for query results\n",
    "pred_output = res['QueryExecution']['ResultConfiguration']['OutputLocation']\n",
    "pred_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = pd.read_csv(pred_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I originally sampled 100k rows to preseve some memory and sorted conversions to the top so you get a lot of conversions to test for but now re-imaged the notebook in a 32GB ram machine so it should handle the full 3.7m rows.  To run the full data set, you would just want to spin this up in a notebook with more memory.  I initially ran this in a notebook with 8GB of memory and 200k rows crapped out when I tried to one hot encode it with get_dummies...so I would have had no chance with the full 1.2m rows the first time I ran but now with the larger machine it should handle 3.7m rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_dum = pd.get_dummies(real_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with XGBoost is that since you turned every categorical feature into specific columns if you don't have those columns in the new data you are scoring it won't work.  So to make it work we weed out the features the model is unaware of with a loop comparing it to the original data the model was built off of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = []\n",
    "for i in real_data_dum.columns.tolist():\n",
    "    if i not in test_data.columns.tolist():\n",
    "        columns_to_drop.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to add in any columns that the new data is missing. This could be app_bundles the model has seen but is not in your data you need scored or states, device makes or models et."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = []\n",
    "for i in test_data.columns.tolist():\n",
    "    if i not in real_data_dum.columns.tolist():\n",
    "        columns_to_add.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to add 0's in for those new features that you are adding in that your data to score doesn't have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add\n",
    "values_to_add = [0 for x in columns_to_add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then you drop the superfluous columns the model hasn't seen and add in the columns that the model has seen but your data doesn't have with 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_dum.memory_usage(index=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_dum = real_data_dum.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_dum[columns_to_add] = pd.DataFrame([values_to_add], index=real_data_dum.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally since XGBoost doesn't have a header in the model or the data you send in to score you need to make sure the columns are in the right order so this just re-orders the columns to be the same as the data the model was built off of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_dum = real_data_dum[test_data.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batches to CSV string payloads (notice, we drop the target variable from our dataset first)\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array\n",
    "\n",
    "(Note: this ran pretty quick and I don't doubt could easily chew through a couple million at a time but for massive data sets the batch method  in sagemaker is probably ideal.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(real_data_dum.drop(['conversions'], axis=1).as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check our confusion matrix to see how well we predicted versus actuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=real_data_dum['conversions'], columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So out of the 31 conversions we predicted 21 correctly and 10 incorrectly which is 67% accuracy (not great) but of the non-converting impression groups we predicted 95k correctly and 4881 incorrectly which is 95% accuracy which is pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rd = real_data.sort_values(by=['conversions'], ascending=False).head(100000)\n",
    "real_data['score']  = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data.sort_values(by=['score'], ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save scored full data file in s3 bucket `fb-beeswax` in /brian/xgboost-prediction/ prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "    's3', region_name='us-east-1')\n",
    "csv_buffer = StringIO()\n",
    "real_data.to_csv(csv_buffer, index=False)\n",
    "res = s3.put_object(Body = csv_buffer.getvalue(),\n",
    "                               ContentType='text/csv',\n",
    "                               Bucket='{buzzkey}-beeswax'.format(buzzkey=buzzkey),\n",
    "                               Key = '{prefix}/xgboost-prediction-output/{date}.csv'.format(prefix=prefix, date=dt.datetime.now(tz).strftime('%Y-%m-%d') ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extensions\n",
    "\n",
    "This example analyzed a relatively small dataset, but utilized Amazon SageMaker features such as distributed, managed training and real-time model hosting, which could easily be applied to much larger problems.  In order to improve predictive accuracy further, we could tweak value we threshold our predictions at to alter the mix of false-positives and false-negatives, or we could explore techniques like hyperparameter tuning.  In a real-world scenario, we would also spend more time engineering features by hand and would likely look for additional datasets to include which contain customer information not available in our initial dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Don't forget to delete the endpoint, these are very expensive machines to leave running) Clean-up\n",
    "\n",
    "If you are done with this notebook, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del model_data\n",
    "del train_data\n",
    "del validation_data\n",
    "del test_data\n",
    "del real_data_dum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hard Target CPA Augmentation\n",
    "\n",
    "Pull in most recent data from Athena (source logs) and do CPA calculations, and decision to bid up/down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapped bigquery for the time being.  Using Athena partially because of free credits, but mainly because we need to filter based on tag_id and if the customer has all tag_id's active but only wants to value a couple, Beeswax API can't filter that out, but we can in Athena from the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda  install --yes pandas-gbq --channel conda-forge\n",
    "#!conda install --yes pydata-google-auth google-auth google-auth-oauthlib google-cloud-bigquery --channel conda-forge\n",
    "#!conda install --yes google-cloud-core --channel conda-forge\n",
    "#! conda install --yes pyasn1-modules pyasn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hard_target_query = \"\"\"\n",
    "select\n",
    "size,\n",
    "app_bundle,\n",
    "inventory_source,\n",
    "sum(impressions_x) as impressions,\n",
    "sum(clicks_x) as clicks,\n",
    "sum(conversions_x) as conversions,\n",
    "sum(spend_x) as spend,\n",
    "SUM(conversions_x)/SUM(impressions_x) AS i,\n",
    "avg(i) as avg_i,\n",
    "variance(i) as var_i,\n",
    "stddev(i) as stdev_i,\n",
    "SUM(spend_x)/SUM(impressions_x)*1000 AS k,\n",
    "avg(k) as avg_k,\n",
    "if (SUM(spend_x)/SUM(conversions_x) >= {cpa}, ({cpa}*SUM(conversions_x))/SUM(impressions_x), (SUM(spend_x)/SUM(impressions_x))+2*stddev(k)) *1000 as bid,\n",
    "({cpa}*SUM(conversions_x))/SUM(impressions_x) * 1000 as price_reduce,\n",
    "((SUM(spend_x)/SUM(impressions_x))+2*stddev(k)) *1000 as price_increase,\n",
    "variance(k) as var_k,\n",
    "stddev(k) as stdev_k,\n",
    "SUM(spend_x)/SUM(conversions_x) AS j\n",
    "\n",
    "from\n",
    "\n",
    "(SELECT\n",
    "  date(rx_timestamp),\n",
    "  concat(banner_width, 'x', banner_height) as size,\n",
    "  app_bundle,\n",
    "  inventory_source,\n",
    "  cast(count(*) as double) AS impressions_x,\n",
    "  SUM(clicks) AS clicks_x,\n",
    "  cast(count_if(tag_id in (1,2)) as double) AS conversions_x,\n",
    "  SUM(spend/1000) AS spend_x,\n",
    "  SUM(spend/1000)/count(*) AS k,\n",
    "  cast(count_if(tag_id in ({tag_ids})) as double)/cast(count(*) as double) AS i,\n",
    "  SUM(spend/1000)/count(*) AS j\n",
    "FROM\n",
    "  \"{buzzkey}-{release_version}\".\"winsconversions\"\n",
    "\n",
    "Where\n",
    " (\"date\"(\"rx_timestamp\") BETWEEN (current_date - INTERVAL  '7' DAY) AND current_date)\n",
    "\n",
    "GROUP BY\n",
    "  concat(banner_width, 'x',  banner_height),\n",
    "  app_bundle,\n",
    "  inventory_source,\n",
    "  date(rx_timestamp)) as con\n",
    "  \n",
    "  group by\n",
    "  size,\n",
    "  app_bundle,\n",
    "  inventory_source\n",
    "  \n",
    " order by conversions desc\n",
    "\n",
    "\n",
    "\"\"\".format(buzzkey=buzzkey, release_version=release_version, cpa=cpa, tag_ids=tag_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set database to buzzkey\n",
    "database = '{}-{}'.format(buzzkey, release_version)\n",
    "#set s3 output file for athena query\n",
    "s3_output_prediction = 's3://{}-beeswax/brian/athena/hard-target/{}/'.format(buzzkey, dt.datetime.now(tz).strftime('%Y-%m-%d-%H%M%S'))\n",
    "\n",
    "#run athena query and kick back job id\n",
    "job = run_query(hard_target_query, database, s3_output_prediction)\n",
    "\n",
    "job_id = job['QueryExecutionId']\n",
    "res = client.get_query_execution(QueryExecutionId= job_id)\n",
    "x = 0\n",
    "\n",
    "# wait for athena to return results\n",
    "while res['QueryExecution']['Status']['State'] != 'SUCCEEDED':\n",
    "    print(\"it's been {} seconds\".format(str(x)))\n",
    "    time.sleep(10)\n",
    "    x = x + 10\n",
    "    res = client.get_query_execution(QueryExecutionId= job_id)\n",
    "\n",
    "#set output location for query results\n",
    "hard_target_output_link = res['QueryExecution']['ResultConfiguration']['OutputLocation']\n",
    "hard_target_output = pd.read_csv(hard_target_output_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the size column that compes from Beeswax API to `banner_width` and `banner_height` to match logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hard_target_output[['banner_width', 'banner_height']] = hard_target_output['size'].str.split('x', expand=True)\n",
    "hard_target_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do a left outter join with the `real_data` so that conversion probability is scored at a granular level but bids are generated based on the target CPA as well as CPM and conversion rate variance at the size/app_bundle/inventory_source level which is where prices are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bid_gen = pd.merge(real_data, hard_target_output, how='left', on=['app_bundle', 'inventory_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bid_gen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take the bid from the target cpa calculation and multiply it by the score for that particular cell to either keep the bid (if 1.0) or price reduce it based on the expected performance of that cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bid_gen['value'] = bid_gen['score'] * bid_gen['bid']\n",
    "bid_gen.sort_values(['conversions_x', 'conversions_y', 'spend_x'], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prepare and Push Bid Model to Beeswax\n",
    "\n",
    "Now the data needs to be sanitized, any non-features clipped off and then pushed to Beeswax Bid Models and set live in their process that pushes the bid configs out to their Aerospike servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Beeswax api endpoints and authenticate Beeswax by logging in and setting cookie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cookies = {\n",
    "}\n",
    "\n",
    "data = '{\"email\":\"brian@dataframesystems.com\", \"password\":\"Dunmore1\"}'\n",
    "\n",
    "auth = 'https://{}.api.beeswax.com/rest/authenticate'.format(buzzkey)\n",
    "\n",
    "get_advertisers = 'https://{}.api.beeswax.com/rest/advertiser'.format(buzzkey)\n",
    "\n",
    "save_report = 'https://{}.api.beeswax.com/rest/report_save'.format(buzzkey)\n",
    "\n",
    "report_queue = 'https://{}.api.beeswax.com/rest/report_queue'.format(buzzkey)\n",
    "\n",
    "list_item_bulk = 'https://{}.api.beeswax.com/rest/list_item_bulk'.format(buzzkey)\n",
    "\n",
    "get_lists = 'https://{}.api.beeswax.com/rest/custom_list'.format(buzzkey)\n",
    "\n",
    "bid_model_version = 'https://{}.api.beeswax.com/rest/bid_model_version'.format(buzzkey)\n",
    "\n",
    "update_active_model_version = 'https://{}.api.beeswax.com/rest/bid_model/strict/1'.format(buzzkey)\n",
    "\n",
    "r = requests.post(auth, cookies=cookies, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any non-feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['app_bundle', 'ad_position', 'geo_country', 'platform_browser',\n",
    "       'platform_os', 'rewarded', 'platform_carrier', 'platform_device_make',\n",
    "       'platform_device_model', 'video_player_size', \n",
    "       'content_language', 'banner_width_x', 'banner_height_x', 'inventory_source', 'value']\n",
    "dz = bid_gen.drop(bid_gen.columns.difference(cols_to_keep), axis=1)\n",
    "dz.rename(columns={'banner_width_x': 'banner_width', 'banner_height_x' : 'banner_height'}, inplace=True)\n",
    "dz.sort_values(['value'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill any bids in the `value` column with a 0 if NaN so that those don't get the default bid for the line item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz['value'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix issue where no size or rewarded produces -1 which Beeswax Models process doesn't accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['banner_width', 'banner_height', 'rewarded']\n",
    "for col in cols:\n",
    "   dz[col] = dz[col].apply(lambda x: int(x) if x == x else \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the `score` column as `value` to match BW Models spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dz.rename(columns={'score':'value'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all -1 cells with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz.replace(-1, \"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all 0 cells in `ad_position` as it doesn't match the standard enum choices in RTB spec.  Not sure why it's coming through..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz.replace({'ad_position' : {'0' : np.nan}}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the memory the total file will take up and then decide the number of partitions to get ~10MB files so that BW Model upload process can parallelize and suck in faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dz = dz.query(\"value == 0\")\n",
    "#len(dz)\n",
    "cut_spend = bid_gen.query(\"value==0\")['spend_x'].sum()\n",
    "kept_spend = bid_gen['spend_x'].sum()\n",
    "print('spend cut from 0 bids is ${}'.format(cut_spend))\n",
    "print('original spend before hard target augmentation is ${}'.format(kept_spend))\n",
    "print('spend after 0 bid cells is ${}'.format(kept_spend-cut_spend))\n",
    "print('efficiency gain is {}%'.format(cut_spend/kept_spend*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_size = dz.memory_usage(index=True).sum()\n",
    "partitions = round(file_size/1024/1024/10)\n",
    "if partitions < 1:\n",
    "    partitions = 1\n",
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate an S3 bucket that Beeswax owns that they expect us to upload files to.  This is a specific role they enabled for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "    's3', aws_access_key_id='AKIAJKXTYAFKSXDNEWKQ',\n",
    "    aws_secret_access_key='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', region_name='us-east-1')\n",
    "s3resource = boto3.resource('s3', aws_access_key_id='AKIAJKXTYAFKSXDNEWKQ',\n",
    "    aws_secret_access_key='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', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk prediction file into n number of partitions and push into Beeswax's S3 bucket as well as set ownership of each object so they can access it and return an array of keys for use in the manifest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = partitions\n",
    "s3_array = []\n",
    "for id, dz_i in  enumerate(np.array_split(dz, number_of_chunks)):\n",
    "    csv_buffer = StringIO()\n",
    "    dz_i.to_csv(csv_buffer, sep='|', index=False)\n",
    "    bucket = 'beeswax-data-us-east-1'\n",
    "    prefix = 'bid_models/{}/predictions/'.format(buzzkey)\n",
    "    key = '{}_{}_file{id}.csv'.format(dt.datetime.now(tz).strftime('%Y-%m-%d'), '{}-impression-group-prediction'.format(buzzkey), id=id )\n",
    "    res = s3.put_object(Body = csv_buffer.getvalue(),\n",
    "                               ContentType='text/csv',\n",
    "                               Bucket= bucket,\n",
    "                               Key = prefix + key)\n",
    "    s3resource.Object(bucket, prefix+key).Acl().put(ACL='bucket-owner-full-control')\n",
    "    s3_array.append('s3://{}/{}{}'.format(bucket, prefix, key))\n",
    "s3_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key)\n",
    "print(prefix+key)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the manifest json by using the columns from the `dz` dataframe after dropping the value column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmanifest = dz.drop(['value'], axis=1)\n",
    "manifest = {\n",
    "    'model_predictions' : \n",
    "        s3_array\n",
    "    ,\n",
    "    'metadata' : {\n",
    "        'fields' : dmanifest.columns.tolist()\n",
    "    }\n",
    "}\n",
    "print(manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the manifest json file to Beeswax s3 bucket where they expect it and set permissions so they can access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_manifest = s3resource.Object(bucket,'bid_models/{buzzkey}/customer_manifests/manifest_{key}.json'.format(buzzkey=buzzkey, key=key))\n",
    "obj_manifest.put(Body=json.dumps(manifest))\n",
    "obj_manifest.Acl().put(ACL='bucket-owner-full-control')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the manifest path so we can use it later in the api to direct the right Model to the right manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_path = 's3://{}/{}'.format(bucket, 'bid_models/{buzzkey}/customer_manifests/manifest_{key}.json'.format(buzzkey=buzzkey, key=key))\n",
    "manifest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model json payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"active\" : True,\n",
    "    \"bid_model_id\" : 1,\n",
    "    \"bid_model_version_name\" : '{}_{}'.format(dt.datetime.now(tz).strftime('%Y-%m-%d-%H%M%S'), '{}-impression-group-prediction'.format(buzzkey) ),\n",
    "    \"manifest_s3_path\" : manifest_path\n",
    "}\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post model json payload to Beeswax api. This creates a new version of the bid model and points to the manifest with instructions on which files to upload with the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_model = requests.post(bid_model_version, cookies=r.cookies, data=json.dumps(model))\n",
    "upload_model.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put request to Beeswax api to update the version of the model to activate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version_data = {\n",
    "    \"bid_model_id\" : 1,\n",
    "    \"active\" : True,\n",
    "    \"current_version\" : upload_model.json()['payload']['id']\n",
    "}\n",
    "\n",
    "update_version = requests.put(update_active_model_version, cookies=r.cookies, data=json.dumps(model_version_data))\n",
    "\n",
    "update_version.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_id, dataframe_agg, buzzkey):\n",
    "    dataframe = dataframe_agg.reset_index()\n",
    "    cols = ['banner_width', 'banner_height']\n",
    "    #for col in cols:\n",
    "        #dataframe[col] = dataframe[col].apply(lambda x: int(x) if x == x else \"\")\n",
    "    file_size = dataframe.memory_usage(index=True).sum()\n",
    "    partitions = round(file_size/1024/1024/10)\n",
    "    if partitions < 1:\n",
    "        partitions = 1\n",
    "    print('there will be {} partitions'.format(partitions))\n",
    "    number_of_chunks = partitions\n",
    "    s3_array = []\n",
    "    for id, dz_i in  enumerate(np.array_split(dataframe, number_of_chunks)):\n",
    "        csv_buffer = StringIO()\n",
    "        dz_i.to_csv(csv_buffer, sep='|', index=False)\n",
    "        bucket = 'beeswax-data-us-east-1'\n",
    "        prefix = 'bid_models/{buzzkey}/predictions/'.format(buzzkey=buzzkey)\n",
    "        key = '{}_{}_file{id}.csv'.format(dt.datetime.now(tz).strftime('%Y-%m-%d'), '{buzzkey}-impression-group-prediction'.format(buzzkey=buzzkey), id=id )\n",
    "        res = s3.put_object(Body = csv_buffer.getvalue(),\n",
    "                                   ContentType='text/csv',\n",
    "                                   Bucket= bucket,\n",
    "                                   Key = prefix + key)\n",
    "        s3resource.Object(bucket, prefix+key).Acl().put(ACL='bucket-owner-full-control')\n",
    "        s3_array.append('s3://{}/{}{}'.format(bucket, prefix, key))\n",
    "    s3_array\n",
    "    dmanifest = dataframe.drop(['value'], axis=1)\n",
    "    manifest = {\n",
    "        'model_predictions' : \n",
    "            s3_array\n",
    "        ,\n",
    "        'metadata' : {\n",
    "            'fields' : dmanifest.columns.tolist()\n",
    "        }\n",
    "    }\n",
    "    print(manifest)\n",
    "    obj_manifest = s3resource.Object(bucket,'bid_models/{buzzkey}/customer_manifests/manifest_{key}.json'.format(buzzkey=buzzkey, key=key))\n",
    "    obj_manifest.put(Body=json.dumps(manifest))\n",
    "    obj_manifest.Acl().put(ACL='bucket-owner-full-control')\n",
    "    manifest_path = 's3://{}/{}'.format(bucket, 'bid_models/{buzzkey}/customer_manifests/manifest_{key}.json'.format(buzzkey=buzzkey, key=key))\n",
    "    model = {\n",
    "        \"active\" : True,\n",
    "        \"bid_model_id\" : model_id,\n",
    "        \"bid_model_version_name\" : '{}_{}'.format(dt.datetime.now(tz).strftime('%Y-%m-%d-%H%M%S'), '{buzzkey}-impression-group-prediction'.format(buzzkey=buzzkey) ),\n",
    "        \"manifest_s3_path\" : manifest_path\n",
    "    }\n",
    "    upload_model = requests.post(bid_model_version, cookies=r.cookies, data=json.dumps(model))\n",
    "    print(upload_model.json())\n",
    "    model_version_data = {\n",
    "        \"bid_model_id\" : model_id,\n",
    "        \"active\" : True,\n",
    "        \"current_version\" : upload_model.json()['payload']['id']\n",
    "    }\n",
    "    \n",
    "    update_version = requests.put(update_active_model_version, cookies=r.cookies, data=json.dumps(model_version_data))\n",
    "    \n",
    "    print(update_version.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tier_1 = dz.groupby(['banner_width', 'banner_height', 'app_bundle', 'inventory_source', 'platform_device_make']).agg({'value': lambda x: np.nanpercentile(x, q = 99)})\n",
    "\n",
    "print(len(d_tier_1))\n",
    "d_tier_1.reset_index().sort_values(by='value', ascending=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tier_2 = dz.groupby(['banner_width', 'banner_height', 'app_bundle', 'inventory_source']).agg({'value': lambda x: np.nanpercentile(x, q = 99.8)})\n",
    "\n",
    "print(len(d_tier_2))\n",
    "d_tier_2.reset_index().sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tier_3 = dz.groupby(['banner_width', 'banner_height', 'inventory_source']).agg({'value': lambda x: np.nanpercentile(x, q = 99.99)})\n",
    "print(len(d_tier_3))\n",
    "d_tier_3.reset_index().sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(2, d_tier_1, buzzkey)\n",
    "load_model(3, d_tier_2, buzzkey)\n",
    "load_model(4, d_tier_3, buzzkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Temp Work - Ignore Below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
